# üöÄ Starter Agent Scaffold

A **compliance-ready** template for building new AI agents. Clone this folder, rename it, and start building ‚Äî all organizational standards are baked in.

---

## üìÅ Folder Structure

```
starter_agent_scaffold/
‚îÇ
‚îú‚îÄ‚îÄ agent_spec.yaml                ‚Üê Agent identity, LLM config, integrations, guardrails
‚îú‚îÄ‚îÄ requirements.txt               ‚Üê Python deps (uncomment providers/integrations you need)
‚îú‚îÄ‚îÄ README.md                      ‚Üê This file
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    ‚Üê FastAPI entry point (/health, /agent/chat + integration routers)
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  ‚Üê Env-based config loader (Pydantic BaseSettings)
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                   ‚Üê Core agent: memory ‚Üí tools ‚Üí LLM ‚Üí response
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/                  ‚Üê LLM Provider Abstraction Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_provider.py       ‚Üê Abstract base class + LLMResponse dataclass
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py     ‚Üê OpenAI (GPT-4o, GPT-4o-mini, o1, o3-mini)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_provider.py     ‚Üê Google Gemini (2.5-pro, 2.5-flash, 2.0-flash)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_provider.py  ‚Üê Anthropic Claude (Sonnet, Haiku, Opus)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_client.py         ‚Üê Unified LLM client + factory (auto-detects from spec)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ integrations/              ‚Üê External Platform Connectors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slack_integration.py   ‚Üê Slack bot (Bolt framework, Socket Mode)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ whatsapp_integration.py‚Üê WhatsApp Cloud API webhook
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ webhook_integration.py ‚Üê Generic webhook with HMAC + callback support
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ system_prompt.txt      ‚Üê Versioned system prompt
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ example_tool.py        ‚Üê Example tool (calculator)
‚îÇ   ‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.py      ‚Üê Session memory (mock ‚Üí swap for Redis)
‚îÇ   ‚îî‚îÄ‚îÄ models/                    ‚Üê Pydantic schemas
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py              ‚Üê Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration_tests/
‚îÇ       ‚îî‚îÄ‚îÄ test_flow.py           ‚Üê E2E flow test with mocked LLM
‚îÇ
‚îú‚îÄ‚îÄ evaluation/                    ‚Üê Benchmark datasets & scoring
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile                 ‚Üê Production container
‚îú‚îÄ‚îÄ ci/
‚îÇ   ‚îî‚îÄ‚îÄ ci_pipeline.yaml          ‚Üê GitHub Actions CI/CD
‚îî‚îÄ‚îÄ docs/                          ‚Üê Agent-specific docs
```

---

## ‚ö° Quick Start

```bash
# 1. Copy and rename
cp -r starter_agent_scaffold/ my-new-agent/
cd my-new-agent/

# 2. Edit agent_spec.yaml ‚Äî set agent_name, provider, model, etc.

# 3. Install core deps
pip install -r requirements.txt

# 4. Install your LLM provider
pip install openai          # For OpenAI
# pip install google-generativeai  # For Gemini
# pip install anthropic      # For Anthropic

# 5. Configure secrets (.env file)
echo "OPENAI_API_KEY=sk-..." > .env

# 6. Validate spec
python ../agent_spec_validator.py agent_spec.yaml

# 7. Run
python app/main.py
# ‚Üí http://localhost:8000/health
# ‚Üí POST http://localhost:8000/agent/chat
```

---

## ü§ñ Multi-Provider LLM Support

Switch providers by changing one line in `agent_spec.yaml`:

```yaml
# OpenAI
llm_provider:
  name: openai
  model: gpt-4o-mini

# Google Gemini
llm_provider:
  name: gemini
  model: gemini-2.5-flash

# Anthropic Claude  
llm_provider:
  name: anthropic
  model: claude-sonnet-4-20250514
```

The `LLMClient` auto-detects the provider from the spec and routes through the correct implementation. All providers return a standardized `LLMResponse` with:
- `output` ‚Äî Generated text
- `tokens_input` / `tokens_output` ‚Äî Token counts
- `cost_estimate` ‚Äî USD cost based on model-specific pricing
- `model` / `error` ‚Äî Metadata

### Adding a New Provider

1. Create `app/services/my_provider.py` extending `BaseLLMProvider`
2. Implement `generate()` and `get_provider_name()`
3. Register it in `llm_client.py`'s `create_provider()` factory

---

## üîå Integrations

### Slack Bot

```bash
pip install slack_bolt
```

```env
SLACK_BOT_TOKEN=xoxb-...
SLACK_SIGNING_SECRET=...
SLACK_APP_TOKEN=xapp-...   # For Socket Mode
```

```bash
python -m app.integrations.slack_integration
```

Supports: `@mention` in channels + direct messages.

---

### WhatsApp (Meta Cloud API)

```env
WHATSAPP_API_TOKEN=...
WHATSAPP_VERIFY_TOKEN=my-verify-token
WHATSAPP_PHONE_NUMBER_ID=...
```

Webhook automatically registered at `/webhook/whatsapp`. Point Meta's webhook config to your domain.

---

### Generic Webhook

Any system can POST to `/webhook/inbound`:

```json
{
  "input": "Hello, agent!",
  "session_id": "external-session-123",
  "callback_url": "https://my-system.com/callback"  
}
```

Optional HMAC-SHA256 verification via `X-Webhook-Signature` header + `WEBHOOK_SECRET` env var.

---

## üîë Key Files

| File | Purpose |
|------|---------|
| `agent_spec.yaml` | Agent identity, LLM provider, integrations, guardrails |
| `app/services/llm_client.py` | Unified LLM client with provider auto-detection |
| `app/services/base_provider.py` | Abstract interface all providers implement |
| `app/integrations/slack_integration.py` | Slack bot via Bolt + Socket Mode |
| `app/integrations/whatsapp_integration.py` | WhatsApp Cloud API webhook handler |
| `app/integrations/webhook_integration.py` | Generic webhook with HMAC + callback |
| `app/agent.py` | Core agent logic |
| `app/config.py` | Environment-based settings (all secrets) |

---

## üìè Compliance

This scaffold conforms to:
- [AI Agents Specification](../docs/ai-agents-specification.md) ‚Äî Architecture & folder structure
- [Observability Framework](../docs/central-observability-framework-specification.md) ‚Äî Logging & metrics
- [Risk Blueprint](../docs/risk-and-compliance-blueprint.md) ‚Äî Security & guardrails
